---
layout: post
title:  "The P versus NP conundrum"
date:   2018-15-07
excerpt: "The million dollar question"
image: "/images/pic12.png"
---
The power of computation has given us the ability to solve problems that we would otherwise have never been able to tackle – but in this article I hope to demonstrate the still existent shortcomings of computation, which build up the foundation of the P versus NP problem. Let us first briefly delve into the world of algorithm time complexity classification before I attempt to explain the problem:

At a high level, the factors that affect how long it takes (the time period) for an algorithm that performs a certain task to be executed include the number of elements or inputs the algorithm has to manipulate and handle, as well as the actual size of these elements. For example, imagine an algorithm that prints out each item in an array – the larger the array, the larger the number of items to print out, and hence the longer the time period of the algorithm. In fact, we can say that the time period of this algorithm is proportional to the number of items in the array; the time period of the algorithm increases linearly in tandem with an increase in the size of the array. In big O notation, we can say that this algorithm is of time complexity O(n) – that is, its time period increases at an order of magnitude proportional to n, with n being the number of items in the array. Now let us take this same algorithm, but this time, change it such that every time it prints an item from the array, it prints that item n times (with n being the size of the array). Hence, if an array is of size 10, then in total 100 items will be printed (as every time an item is printed, it is printed 10 times, thus resulting in 10x10 items being printed). If there are n items in the array, n<sup>2</sup> items are printed; and thus this algorithm will have a time complexity of O(n<sup>2</sup>). If we actually these two time complexities we have derived, we can see how the time period increases as a result of n increasing. Let us suppose, in the first example with linear time complexity, that it takes 1 second to print out an item in the array. If we say the proportionality constant is 1 to maintain simplicity, then it takes n seconds to print out n items. If n is 1, it takes 1 second. If n is 1000, it takes 1000 seconds – or otherwise just 16 minutes. From this we can see that the rate of increase of the time period in accordance with n isn’t so drastic that it becomes impossible to compute. Similarly, with O(n<sup>2</sup>), if n is 1 it takes 1 second, and if n is 1000 it takes 1000<sup>2</sup> seconds – or otherwise 256 minutes. The rate of increase is clearly higher but still not so high that it becomes impossible to compute. Another thing to point out is that even 1 second to print out an item is a gross exaggeration – most modern computers will be much more faster. In fact, most computer scientists agree that any time complexity of O(n<sup>k</sup>), where k>0, can generally be computed in a practical amount of time, and these time complexities are more generally classified as **polynomial** time. And here we have our link to the P versus NP problem – a P problem is one that can be solved using an algorithm that can be executed in polynomial time – or in other words has a time complexity of O(n<sup>k</sup>). Thus these P problems are often deemed to be solvable with a reasonable level of processing power and a reasonable period of time. However, this leaves the NP side – which as you may have guessed, are problems that *cannot* be solved in polynomial time, and thus generally cannot be solved with a reasonable level of processing power and time.

For an algorithm to not be able to be executed in polynomial time, its time period follows an **exponential pattern**. That is, as the number of inputs into the algorithm increase, the time taken for the algorithm to be executed exponentially increases. Using big O notation, this time complexity can be modelled as O(k<sup>n</sup>), where k>1 and the number of inputs is equal to n. But why exactly would this time complexity prevent a solution being obtained in a reasonable amount of time? Let us analyse it further.

Suppose we take k to be 2, and the number of inputs, n, being 1 (we will also take the constant or proportionality to be 1). If we assume it takes 1 second to compute a single input, then for 1 input it will take 2<sup>1</sup> = 2 seconds to compute. In the previous example, if we raised the number of inputs to 1000, the time taken would substantially increase, but not so much that it becomes unreasonable to do. However, in this example if we raise n to 1000, the amount of time it would take would be longer than the time elapsed since the Big Bang. Here, it should be quite obvious to see that a problem that is solved using this algorithm would not be able to solved in a reasonable period of time as n increases, and thus this problem would be classified as an NP problem.

However, there is a point I have not yet mentioned – if a solution is given to an NP problem, it can be easily checked to see if it is correct. An example of this is in the NP problem of prime factorisation. It’s a mathematical rule that any integer can be expressed as a product of its prime factors, although for large integers the time period required to find these prime factors exponentially increases. However, if a solution is given, then it can easily be verified by simply multiplying the numbers together, and seeing if the answer is equal to the original integer. This can be done in polynomial time, which has then led people to question that if a solution to an NP problem can be verified in polynomial time, then shouldn’t the problem itself be able to be solved in polynomial time, using a more efficient algorithm that we have not yet discovered? And if this is true, then that would mean all NP problems are just P problems, of which an efficient algorithm to solve them has just not yet been discovered – and hence P=NP. What’s remarkably fascinating is that, while P=NP has not been proved, P not equalling NP has also not been proved! And thus the P vs NP problem still remains today one of the most baffling questions in computing theory, so much so that the Clay Mathematics institute is offering one million dollars to whoever can solve it. There is still a significant amount more regarding the problem, including recent approaches and discoveries made, most notably advances in Quantum Computing that are possibly starting to unravel algorithms that can solve NP problems in polynomial time, and thus hinting P does indeed NP – but I will attempt to cover all this in a continuation post later.