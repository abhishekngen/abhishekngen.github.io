---
layout: post
title:  "Wonders of the Turing Machine"
date:   2018-08-06
excerpt: "The blueprint that started it all"
image: "/images/pic08.png"
---

In the modern world, the actual entity that is a computer can often be characterised by several vague definitions due to the sheer number of features now available on a typical "computer". However, it is important to establish the fundamental definition of a computer – and that is essentially a device which is capable of receiving a set of data, performing a variable set of instructions on the data, then producing a result. This, is in fact, the essence of the Turing Machine – a hypothetical model developed by Alan Turing that paved the path towards the development of modern computing systems. 

This incredible blueprint was conjured while Alan Turing was visiting Von Neumann, another famous computer scientist. Essentially, the initial concept consisted of a machine in which a paper tape that was infinitely long was fed into it. This paper tape was partitioned into squares, in which each square would either contain a "1" or be blank, effectively corresponding to a "0" – the first implementation of binary code. These sequences of 1s and 0s, as done in even modern binary, are capable of representing any piece of information – for example, the letter "A" can be represented as "10", "B" could be "11", and by continuously stretching the number of binary bits used it is possible to represent any alphanumeric character. Thus the tape is capable of presenting any data to the machine to be computed. 

The actual machine that computes the data then exhibits the behaviour of something known as a finite state machine in order to actually carry out a set of instructions on the data. What this means is that the machine will initially start out in a certain state that is pre-programmed into the machine. When the tape is fed into the machine, the machine, in this initial state, reads the symbol contained in the first square on the tape, that symbol being either a 1 or blank (effectively a "0"). Dependent on the state the machine is currently in, it either erases the 1 within the square, writes a 1 to the square, or does nothing. Then two other fundamental steps occur – the value on the square determines whether the machine will move either one square to the right or left. According to the concept initially laid out by Turing, the machine is indeed only restricted to moving either right or left, and whether it moves right or left is not only dependent on what value it reads, but also what state it is currently in. The value on the square also just as importantly determines the next state the machine will transition to, or of course whether the machine should remain within the same state. However, this state transition is dependent on the state the machine is currently in. This is much better illustrated in a diagram:

<span class="image center"><img src="{{ "/images/pic09.png" | absolute_url }}" alt="" /></span>

As you can see, if the machine starts of in state 1, and then reads an input of 1 on the tape, it will remain in state 1. However, if it reads a blank input (a "0"), it will transition to state 2. From there, it can be seen it will now undergo different transitions based on the input. And as I said, in each different state, the machine will be programmed to either write or erase a 1 on the current square, or do nothing. 

Now, the machine eventually has to stop going back and forth across the tape, editing values, and this is done when the machine reaches what is called an **accepting state**. When this accepting state has been reached, the machine can stop editing the tape and output it. And the symbols that are now left on the tape represent the computed data! For example, imagine if I were to set up a Turing machine such that it would add together two numbers. I would have to engineer a state transition model, similar to the one above but naturally, far more complex, as well as a set of instructions regarding how the machine should respond to each inputted binary value dependent on which state it is in (this combination of instructions for a Turing Machine to compute a certain problem is known as an **action table**), that would then enable the machine to read two numbers on the tape and add them together. I would then input a tape containing two numbers encoded in binary, and once the Turing machine has finished processing it, the binary values left on the tape would represent the value of the addition!

However, there are certain shortcomings here that can be clearly noted. The most obvious one is the usage of an "infinite" tape, although this was simply included in the hypothetical model to enable the Turing machine to theoretically solve any "computable" problem regardless of size. To some extent this is also why modern computers of varying "power" exist, and you can treat the tape size as similar to the size of modern computers' RAM and memory. 

The more intrinsic shortcoming is the fact that each Turing Machine needs to have a pre-programmed set of instructions (inclusive of a state transition model) in order to compute a result for a problem. This means that each Turing Machine is only capable of solving and computing a specific task, and can only perform a pre-defined single set of instructions upon data, and thus fails to meet the modern definition of a computer, which is capable of performing a **variable** set of instructions upon data, including letting the user dynamically instruct the computer on how to deal with a set of data. However, Turing soon identified a relatively simple method to enable this, and dubbed this new machine as the **Universal** Turing Machine. 

The way in which the Universal Turing Machine works is fairly similar to the way in which a standard Turing Machine works, except instead of the machine being pre-programmed with a specific action table (consisting of a state transition model and instructions on how to respond to various binary inputs), the user can actually provide the action table for the Turing Machine to use by encoding it on tape! This can be manifested in multiple ways – for example producing the Turing Machine such that is actually reads 2 tapes, or perhaps instead reads action tables off the same tape, where an action tape can be distinguished on the tape by use of further binary codes. The implementation of this addition effectively means the Universal Turing Machine is the basis for all modern computers.

Upon analysis of this machine, Turing was then able to come up with a few particularly interesting deductions. In his paper "On Computable Numbers", he establishes that there still exist problems that supposedly cannot be computed, which has then branched into several arguments still being disputed today, including the notable P vs NP argument, of which I will probably cover in a future post. A more interesting deduction, however, is the theory that a Turing Machine will never be able to predict how many steps it will take to solve a particular problem. In other words, if you were to ask a Turing Machine to add together two numbers, it would not be able to tell you **how many steps** it actually take, and thus how long it will take, to add together those numbers. Consequently it will also not be able to tell you if it ever will solve the problem – or in other words, if the Turing Machine will enter the accepting state and "halt", or whether it will infinitely loop on and never actually terminate and reach an accepting state. This was then eloquently proved by Turing:

Suppose there exists a Turing Machine ***A*** that can take in a program ***p*** which itself has been given a certain input, and can then determine whether ***p*** enters an accepting state and "halts", and thus solves its problem, or never reaches an accepting state and thus never "halts". ***A*** is then able to show this by it itself having two accepting states - if ***p*** terminates, ***A*** enters its first accepting state, and if  ***p*** doesn’t, ***A*** enters its second accepting state. Now suppose a second Turing Machine, ***B***, that runs ***A*** on a certain program ***p***, and then if ***A*** declares that ***p*** does halt and thus does eventually reach an accepting state, then ***B*** actually loops forever and does not halt, and if ***B*** declares ***p*** does not halt, then ***B*** does halt. As bizarre as this contraption is, it is then used in the ingenious proof by contradiction. Suppose we feed ***B*** as an input into another ***B***, and then feed this into ***A*** – essentially producing ***A(B(B))***. Now there are only two outcomes of this – either ***A*** says its input does halt, or it doesn't halt. Now, if we examine the scenario where ***A*** says ***B(B)*** does halt, then referring back to the definition of ***B***, if ***B*** halts, it means its input does not halt. However, as ***B*** is its input, this is saying that ***B*** both halts and doesn't halt, which is a contradiction. In the other outcome, if ***A*** says ***B(B)*** doesn't halt, this would mean the input of ***B*** does halt – however, as ***B*** is its input, ***B*** cannot both halt and not halt, and thus there exists another contradiction. Therefore this proves that a Turing Machine cannot dictate whether it will ever be able to solve a problem and 'halt', and thus Turing Machines are incapable of predicting how long it will take to solve a certain problem.